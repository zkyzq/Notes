如何高效实现Conv1D？这里给出一种采用mxnet.ndarray.Convolution中num_group=emb_dim的技巧。

Conv2D输入 卷积核 输出格式分别如下：
https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html?highlight=ndarray.conv#mxnet.ndarray.Convolution
input: (batch_size, channel, height, width)
weight: (num_filter, channel, kernel[0], kernel[1])
bias: (num_filter,)
out: (batch_size, num_filter, out_height, out_width)
If num_group is larger than 1, denoted by g, then split evenly into g parts: the input data along the channel axis and
weight along the zeroth dimension. Next compute the convolution on the i-th part of the data with the i-th weight part. 
The output is obtained by concatenating all the g results at axis==1.
总结上一段如下，
正常2D卷积：
input: (B, C, H, W)        batch,in_channel,height,width
weight: (Co, C, K0, K1)    out_channel,in_channel,kernel[0],kernel[1]
bias: (Co,)
out: (B, Co, Ho, Wo)
带group的2D卷积：
input: (B, GC, H, W)        batch,group*in_channel,height,width
weight: (GCo, C, K0, K1)    group*out_channel,in_channel,kernel[0],kernel[1]
bias: (GCo,)
out: (B, GCo, Ho, Wo)

这样我们把数据reshape成：
input: (batch_size, emb_dim, in_size, 1)
weight: (num_filter*emb_dim, 1, kernel0, 1)
bias: (num_filter*emb_dim,)

验证代码如下：
batch_size, emb_dim, in_size, num_filter, kernel0 = 1,3,4,2,2
x = nd.ones((batch_size, emb_dim, in_size, 1))
# W = nd.ones((num_filter*emb_dim, 1, kernel0, 1))
W = nd.arange(num_filter*emb_dim*kernel0).reshape((num_filter*emb_dim, 1, kernel0, 1))
b = nd.ones((num_filter*emb_dim,))
# out: (batch_size, num_filter*emb_dim, out_height, 1)
out = nd.Convolution(data=x, weight=W, bias=b, kernel=W.shape[2:], 
                num_filter=W.shape[0], num_group=emb_dim)

最后需要做k max pooling的时候直接对输出的out_height axis做topk



