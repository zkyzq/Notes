如何高效实现Conv1D？这里给出一种采用mxnet.ndarray.Convolution中num_group=emb_dim的技巧。

Conv2D输入 卷积核 输出格式分别如下：
https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html?highlight=ndarray.conv#mxnet.ndarray.Convolution
input: (batch_size, channel, height, width)
weight: (num_filter, channel, kernel[0], kernel[1])
bias: (num_filter,)
out: (batch_size, num_filter, out_height, out_width)
If num_group is larger than 1, denoted by g, then split evenly into g parts: the input data along the channel axis and
weight along the zeroth dimension. Next compute the convolution on the i-th part of the data with the i-th weight part. 
The output is obtained by concatenating all the g results at axis==1.
总结上一段如下，
正常2D卷积：
input: (B, C, H, W)        batch,in_channel,height,width
weight: (Co, C, K0, K1)    out_channel,in_channel,kernel[0],kernel[1]
bias: (Co,)
out: (B, Co, Ho, Wo)
带group的2D卷积：
input: (B, GC, H, W)        batch,group*in_channel,height,width
weight: (GCo, C, K0, K1)    group*out_channel,in_channel,kernel[0],kernel[1]
bias: (GCo,)
out: (B, GCo, Ho, Wo)

这样我们把数据reshape成：
input: (batch_size, emb_dim, in_size, 1)
weight: (num_filter*emb_dim, 1, kernel0, 1)
bias: (num_filter*emb_dim,)

验证代码如下：
batch_size, emb_dim, in_size, num_filter, kernel0 = 1,3,4,2,2
x = nd.ones((batch_size, emb_dim, in_size, 1))
# W = nd.ones((num_filter*emb_dim, 1, kernel0, 1))
W = nd.arange(num_filter*emb_dim*kernel0).reshape((num_filter*emb_dim, 1, kernel0, 1))
b = nd.ones((num_filter*emb_dim,))
# out: (batch_size, num_filter*emb_dim, out_height, 1)
out = nd.Convolution(data=x, weight=W, bias=b, kernel=W.shape[2:], 
                num_filter=W.shape[0], num_group=emb_dim)

怎么做一个合理的pooling操作(k max pooling)，topk不合理效果不好，更合理的方式是sumpooling
1. 对输出的out_height axis做topk
2. sumpooling顾名思义就是对先求和，再根据最大值做k max pooling：
pooling 输入：(batch, num_filter*emb_dim, in_size, 1)，即卷积输出
pooling 输出：(batch, num_filter*emb_dim, k, 1)
分步操作如下：
def sumpool(self, x):
    ''' k max pooling indexed by sum
    input: (batch, num_filter*emb_dim, in_size*in_size/2, 1)
    ouput: (batch, num_filter*emb_dim, in_size, 1) '''        
    x = x.split(axis=1, num_outputs=emb_dim)
    # x: (batch, num_filter, in_size*in_size/2, emb_dim)
    x = nd.concat(*x, dim=3)       
    b,c,i,e = x.shape
    k = in_size
    idx = x.sum(axis=3).topk(axis=2, k=k).reshape((-1, k))
    x = x.reshape((-1, e))
    idx = (idx + nd.expand_dims(nd.arange(b*c) * i, 1)).reshape((-1,))
    # x: (batch, num_filter, in_size, emb_dim)
    x = x.take(idx, axis=0).reshape((b, c, k, e))   
    x = x.transpose(axes=(0,2,3,1))
    x = x.reshape((b,k,1,c*e))
    # x: (batch, num_filter*emb_dim, in_size, 1)
    x = x.transpose(axes=(0,3,1,2))
    return x
    



